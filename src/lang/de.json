{
    "ai-pc-ollama.confirmation.title": "Bestätigung",
    "ai-pc-ollama.confirmation.question": "Bist du sicher?",
    "ai-pc-ollama.confirmation.yes": "Ja",
    "ai-pc-ollama.confirmation.no": "Nein",
    "ai-pc-ollama.confirmation.confirmed": "Bestätigt",
    "ai-pc-ollama.confirmation.cancelled": "Abgebrochen",
    "ai-pc-ollama.chatLog.clearConversations": "Alle AI PC Ollama-Gespräche löschen",
    "ai-pc-ollama.chatLog.clearConversationsConfirmation": "Bist du sicher, dass du alle AI PC Ollama-Gespräche löschen möchtest? Dies löscht keine Nachrichten.",
    "ai-pc-ollama.chatLog.removeFromConversation": "Aus AI PC Ollama-Gespräch entfernen",
    "ai-pc-ollama.chatMessage.actorIdNotFound": "Akteur mit ID \"{id}\" nicht gefunden. Wurde er gelöscht?",
    "ai-pc-ollama.chatMessage.actorAliasNotFound": "Akteur mit Alias \"{alias}\" nicht gefunden.",
    "ai-pc-ollama.chatMessage.marker": "Spricht mit {audience}",
    "ai-pc-ollama.chatMessage.multipleActors": "Mehrere Akteure mit Alias \"{alias}\" gefunden: {names}",
    "ai-pc-ollama.chatMessage.triggerWithoutActor": "triggerResponse wurde ohne Akteur aufgerufen.",
    "ai-pc-ollama.chatMessage.noResponse": "Es wurde keine Antwort generiert.",
    "ai-pc-ollama.chatMessage.preambleTooLong": "Das Gespräch hat gerade erst begonnen, ist aber bereits zu lang für das Modell. Dies liegt wahrscheinlich daran, dass die Präambel zu lang ist. Bitte kürze die Präambel oder wechsle zu einem anderen Modell mit einer größeren Kontextlänge.",
    "ai-pc-ollama.chatMessage.truncatingMessage": "Dieses Gespräch, welches {messageCount} Nachrichten umfasst, ist zu lang für das Modell und wird gekürzt. Um dies in Zukunft zu verhindern, kannst du entweder das Modell wechseln oder das Gespräch verkürzen, indem du vorherige Nachrichten aus dem Gespräch entfernts.",
    "ai-pc-ollama.chatMessage.noPreamble": "Für Akteur {name} wurde keine Präambel hinterlegt.",
    "ai-pc-ollama.info.title": "Einen Moment bitte",
    "ai-pc-ollama.info.message": "Dies kann eine Weile dauern, während der FoundryVTT nicht reagiert.",
    "ai-pc-ollama.llm.noValue": "Kein Wert für {parameterName} gefunden. Verwende Standardwert {value}.",
    "ai-pc-ollama.llm.noDefaultValue": "Kein Standardwert für {parameterName} gefunden.",
    "ai-pc-ollama.llm.preparingModel": "Modell \"{model}\" wird vorbereitet...",
    "ai-pc-ollama.llm.preparingTokenizer": "Tokenizer für \"{model}\" wird vorbereitet...",
    "ai-pc-ollama.llm.generatingResponse": "Antwort von {actorName} wird generiert...",
    "ai-pc-ollama.llm.localLlmError": "Ein Fehler ist während der Textgenerierung mit dem lokalen LLM aufgetreten: {error}",
    "ai-pc-ollama.llm.openAiError": "Antwort von OpenAI konnte nicht abgerufen werden: {error}",
    "ai-pc-ollama.prefix.invalid": "Das Präfix \"{prefix}\" wird nicht erkannt und daher ignoriert.",
    "ai-pc-ollama.settings.model": "Großes Sprachmodell",
    "ai-pc-ollama.settings.modelDescription": "Das Standardmodell, welches von AI PC Ollama-Akteuren verwendet wird, um Antworten zu generieren.\nLokale Modelle laufen in deinem Browser oder der FoundryVTT-Instanz, während OpenAI-Modelle auf einem anderen Server laufen.\nOpenAI-Modelle sind viel schneller und liefern meist bessere Ergebnisse, erfordern jedoch einen API-Schlüssel, um zu funktionieren.",
    "ai-pc-ollama.settings.apiKey": "OpenAI API-Schlüssel",
    "ai-pc-ollama.settings.apiKeyDescription": "Falls du OpenAI-Modelle verwenden möchtest, musst du hier deinen API-Schlüssel angeben.\nZusätzlich muss dein Konto ein positives Guthaben haben.",
    "ai-pc-ollama.settings.minNewTokens": "Minimale Anzahl neuer Token",
    "ai-pc-ollama.settings.minNewTokensDescription": "In großen Sprachmodellen bestimmt die Anzahl der Token die Länge des generierten Textes.\nUm sehr kurze Antworten zu vermeiden, kannst du hier eine minimale Anzahl von Token festlegen. Hinweis: Dieser Parameter wird nur bei lokalen Modellen berücksichtigt.",
    "ai-pc-ollama.settings.maxNewTokens": "Maximale Anzahl neuer Token",
    "ai-pc-ollama.settings.maxNewTokensDescription": "Um übermäßig lange Antworten zu vermeiden, kannst du hier eine maximale Anzahl von Token festlegen.",
    "ai-pc-ollama.settings.repetitionPenalty": "Wiederholungsstrafe / Häufigkeitsstrafe",
    "ai-pc-ollama.settings.repetitionPenaltyDescription": "Die Wiederholungsstrafe ist eine Zahl, welche die Wahrscheinlichkeit anpasst, dass ein Token, welches bereits generiert wurde, erneut generiert wird.\nHöhere Werte verringern die Wahrscheinlichkeit von Wiederholungen, negative Werte erhöhen sie.",
    "ai-pc-ollama.settings.temperature": "Temperatur",
    "ai-pc-ollama.settings.temperatureDescription": "Große Sprachmodelle generieren Text, indem sie aus einer Wahrscheinlichkeitsverteilung über den Wortschatz auswählen.\nDie Temperatur beeinflusst diese Verteilung:\nBei einer Temperatur von 0 wählt das Modell immer das wahrscheinlichste Token, während alle Token für sehr hohe Temperaturen fast gleich wahrscheinlich werden.\nNiedrigere Werte machen das Modell konservativer, höhere Werte machen es kreativer.",
    "ai-pc-ollama.settings.prefix": "Stelle Antworten Folgendes voran",
    "ai-pc-ollama.settings.prefixDescription": "Die Wahl eines nicht-leeren Wertes stellt diesen der generierten Antwort voran. Der <user> Platzhalter wird durch den Namen des aufrufenden Benutzers ersetzt. Das /talk-Makro wird vom Talking Actors FoundryVTT-Modul erkannt.",
    "ai-pc-ollama.shared.unkenninessForNull": "UnKenniness für null Akteur überprüft.",
    "ai-pc-ollama.shared.moduleLoadFailed": "Modul \"{name}\" konnte nicht geladen werden: {error}",
    "ai-pc-ollama.sheet.title": "Bearbeite UnKenniness-Parameter",
    "ai-pc-ollama.sheet.alias": "Alias",
    "ai-pc-ollama.sheet.preamble": "Präambel",
    "ai-pc-ollama.sheet.overwrite": "Globale Parameter überschreiben",
    "ai-pc-ollama.sheet.save": "Speichern",
    "ai-pc-ollama.sheet.settingAliasFailed": "Akteur nicht gefunden, das Alias Setzen ist fehlgeschlagen."
}