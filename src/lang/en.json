{
    "ai-pc-ollama.confirmation.title": "Confirmation",
    "ai-pc-ollama.confirmation.question": "Are you sure?",
    "ai-pc-ollama.confirmation.yes": "Yes",
    "ai-pc-ollama.confirmation.no": "No",
    "ai-pc-ollama.confirmation.confirmed": "Confirmed",
    "ai-pc-ollama.confirmation.cancelled": "Cancelled",
    "ai-pc-ollama.chatLog.clearConversations": "Clear AI PC Ollama conversations",
    "ai-pc-ollama.chatLog.clearConversationsConfirmation": "Are you sure you want to clear all AI PC Ollama conversations? This will not delete any messages.",
    "ai-pc-ollama.chatLog.removeFromConversation": "Remove from AI PC Ollama conversation",
    "ai-pc-ollama.chatMessage.actorIdNotFound": "Actor with id \"{id}\" not found. Has it been deleted?",
    "ai-pc-ollama.chatMessage.actorAliasNotFound": "Actor with alias \"{alias}\" not found.",
    "ai-pc-ollama.chatMessage.marker": "Speaking with {audience}",
    "ai-pc-ollama.chatMessage.multipleActors": "Multiple actors with alias \"{alias}\" found: {names}",
    "ai-pc-ollama.chatMessage.triggerWithoutActor": "triggerResponse was called with no actor.",
    "ai-pc-ollama.chatMessage.noResponse": "No response was generated.",
    "ai-pc-ollama.chatMessage.preambleTooLong": "The conversation has only just begun, but it is already too long for the model. This is likely due to the preamble being too long. Please shorten the preamble or switch to a different model with a larger context size.",
    "ai-pc-ollama.chatMessage.truncatingMessage": "This conversion spanning {messageCount} messages is too long for the model, and will be truncated. To prevent this in the future, you can either switch the model or shorten the conversation by removing previous messages from the conversation.",
    "ai-pc-ollama.chatMessage.noPreamble": "No preamble set for actor {name}.",
    "ai-pc-ollama.info.title": "One moment, please",
    "ai-pc-ollama.info.message": "This may take a while, during which FoundryVTT will be unresponsive.",
    "ai-pc-ollama.llm.noValue": "No value found for {parameterName}. Using default value {value}.",
    "ai-pc-ollama.llm.noDefaultValue": "No default value found for {parameterName}.",
    "ai-pc-ollama.llm.preparingModel": "Preparing model \"{model}\"...",
    "ai-pc-ollama.llm.preparingTokenizer": "Preparing tokenizer for \"{model}\"...",
    "ai-pc-ollama.llm.generatingResponse": "Generating {actorName}'s response...",
    "ai-pc-ollama.llm.localLlmError": "An error occurred during text generation with local LLM: {error}",
    "ai-pc-ollama.llm.openAiError": "Unable to get response from OpenAI: {error}",
    "ai-pc-ollama.prefix.invalid": "The prefix \"{prefix}\" is not recognised and will thus be ignored.",
    "ai-pc-ollama.settings.model": "Large Language Model",
    "ai-pc-ollama.settings.modelDescription": "The default model used by unkenny actors to generate responses.\nLocal models run in your browser or FoundryVTT instance, while OpenAI models run on a remote server.\nOpenAI models are much faster and mostly yield better results, but they require an API key to work.",
    "ai-pc-ollama.settings.apiKey": "OpenAI API Key",
    "ai-pc-ollama.settings.apiKeyDescription": "If you want to use OpenAI models, you need to provide your API key here.\nAdditionally, your account must have a positive balance.",
    "ai-pc-ollama.settings.minNewTokens": "Minimum Number of New Tokens",
    "ai-pc-ollama.settings.minNewTokensDescription": "In large language models, the number of tokens determines the length of the generated text.\nTo avoid very short responses, you can set a minimum number of tokens here. Note: This parameter is only considered in local models.",
    "ai-pc-ollama.settings.maxNewTokens": "Maximum Number of New Tokens",
    "ai-pc-ollama.settings.maxNewTokensDescription": "To avoid overly long responses, you can set a maximum number of tokens here.",
    "ai-pc-ollama.settings.repetitionPenalty": "Repetition Penalty / Frequency Penalty",
    "ai-pc-ollama.settings.repetitionPenaltyDescription": "The repetition penalty is a number that adjusts the likelyhood of a token that has already been generated to be generated again.\nHigher values reduce the likelihood of repetition, negative values increase it.",
    "ai-pc-ollama.settings.temperature": "Temperature",
    "ai-pc-ollama.settings.temperatureDescription": "Large language models generate text by sampling from a probability distribution over the vocabulary.\nTemperature infuences this distribution:\nWith a temperature of 0, the model always chooses the most likely token, while all token become nearly equally likely for very high temperatures.\nLower values make the model more conservative, higher values make it more creative.",
    "ai-pc-ollama.settings.prefix": "Prefix responses with",
    "ai-pc-ollama.settings.prefixDescription": "Choosing some non-empty value will prefix the generated response with it. The <user> placeholder is replaced by the username of the caller. The /talk macro is recognised by the Talking Actors FoundryVTT module.",
    "ai-pc-ollama.shared.unkenninessForNull": "UnKenniness checked for null actor.",
    "ai-pc-ollama.shared.moduleLoadFailed": "Unable to load module \"{name}\": {error}",
    "ai-pc-ollama.sheet.title": "Edit UnKenniness Parameters",
    "ai-pc-ollama.sheet.alias": "Alias",
    "ai-pc-ollama.sheet.preamble": "Preamble",
    "ai-pc-ollama.sheet.overwrite": "Overwrite Global Parameters",
    "ai-pc-ollama.sheet.save": "Save",
    "ai-pc-ollama.sheet.settingAliasFailed": "Actor not found, setting alias failed."
}